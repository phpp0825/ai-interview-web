# evaluation.py
# Evaluate candidate responses via LLM and save evaluation results to a text file

import json
import logging
from typing import List

from .llm_client import LLMClient
from .config import LlamaConfig
from .stt import STTClient, calculate_silence_duration, calculate_audio_duration

logger = logging.getLogger(__name__)

def evaluate_and_save_responses(
    questions: List[str],
    answers: List[str],
    audio_files: List[str],
    output_file: str = "interview_evaluation.txt"
) -> list:
    """
    Evaluate user responses using an LLM and save structured results to a TXT file.
    ë¹ˆ ë‹µë³€ ë° 'ê·¸ë§Œí•˜ê² ìŠµë‹ˆë‹¤' íŠ¸ë¦¬ê±°ë¥¼ ê±´ë„ˆë›°ê³ ,
    ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œë§Œ ì œê³µí•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.
    ê° í‰ê°€ í•­ëª©ì— ëŒ€í•´ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì´ì ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    llm_config = LlamaConfig()
    llm_client = LLMClient(llm_config)
    stt_client = STTClient()

    evaluations = []

    for idx, (question, answer, audio_path) in enumerate(zip(questions, answers, audio_files), start=1):
        # 1) 'ê·¸ë§Œí•˜ê² ìŠµë‹ˆë‹¤' ë©´ì ‘ ì¢…ë£Œ íŠ¸ë¦¬ê±° ê±´ë„ˆë›°ê¸°
        if answer.strip().lower() == "ê·¸ë§Œí•˜ê² ìŠµë‹ˆë‹¤":
            logger.info(f"Skipping evaluation for exit trigger at Q{idx}")
            break

        # 2) ë¹ˆ ë‹µë³€ì— ëŒ€í•´ì„œëŠ” LLM í˜¸ì¶œ ì—†ì´ ë‚®ì€ í‰ê°€ ì²˜ë¦¬
        if not answer.strip():
            try:
                total_time = calculate_audio_duration(audio_path)
                # ë¹ˆ ë‹µë³€ì˜ ê²½ìš° ì „ì²´ê°€ ì¹¨ë¬µìœ¼ë¡œ ê°„ì£¼
                silence = total_time
            except Exception as e:
                logger.warning(f"Audio duration calculation failed for {audio_path}: {e}")
                total_time = 0.0
                silence = 0.0
                
            evaluations.append({
                "question": question,
                "user_answer": "",
                "evaluation": {
                    "relevance":      {"rating": "ë‚®ìŒ", "comment": "ì‘ë‹µì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                    "completeness":   {"rating": "ë‚®ìŒ", "comment": "ì‘ë‹µì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                    "correctness":    {"rating": "ë‚®ìŒ", "comment": "ì‘ë‹µì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                    "clarity":        {"rating": "ë‚®ìŒ", "comment": "ì‘ë‹µì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                    "professionalism":{"rating": "ë‚®ìŒ", "comment": "ì‘ë‹µì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                },
                "recommended_answer": "",
                "total_response_time": total_time,
                "silence_duration":    silence,
                "total_score": 0  # ë¹ˆ ë‹µë³€ì€ 0ì 
            })
            continue

        # 3) LLM í”„ë¡¬í”„íŠ¸: í•œêµ­ì–´ ê°•ì œ, JSON ì˜ˆì‹œë„ í•œê¸€í™” (ì—„ê²©í•œ í‰ê°€ ê¸°ì¤€)
        prompt = (
            "ëª¨ë“  ì¶œë ¥ì€ *ì˜¤ì§ í•œêµ­ì–´*ë¡œë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n"
            "ë‹¹ì‹ ì€ ê¹Œë‹¤ë¡œìš´ IT íšŒì‚¬ì˜ ìˆ™ë ¨ëœ ë©´ì ‘ê´€ì´ì í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
            "ë†’ì€ ìˆ˜ì¤€ì˜ ë‹µë³€ë§Œì„ ì¸ì •í•˜ë©°, ì—„ê²©í•œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n"
            "ì•„ë˜ ë©´ì ‘ ì§ˆë¬¸ê³¼ ì§€ì›ìì˜ ë‹µë³€ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¤ì„¯ ê°€ì§€ ê¸°ì¤€ì— ë”°ë¼ **ì—„ê²©í•˜ê²Œ** í‰ê°€í•˜ê³ ,"
            "ì¶”ì²œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.\n\n"
            "**í‰ê°€ ê¸°ì¤€ (ë§¤ìš° ì—„ê²©í•˜ê²Œ ì ìš©):**\n"
            "1. ê´€ë ¨ì„±: ë‹µë³€ì´ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì •í™•íˆ ë‹¤ë£¨ì—ˆëŠ”ê°€? (ì• ë§¤í•œ ë‹µë³€ì€ ë‚®ìŒ)\n"
            "2. ì™„ì „ì„±: ë‹µë³€ì— í•„ìš”í•œ ëª¨ë“  ìš”ì†Œê°€ êµ¬ì²´ì ìœ¼ë¡œ í¬í•¨ë˜ì—ˆëŠ”ê°€? (ì¼ë°˜ì ì¸ ë‹µë³€ì€ ë‚®ìŒ)\n"
            "3. ì •í™•ì„±: ì •í™•í•œ ì‚¬ì‹¤ê³¼ ë…¼ë¦¬ì— ê¸°ë°˜í•œ ë‚´ìš©ì¸ê°€? (ì¶”ìƒì ì¸ ë‹µë³€ì€ ë‚®ìŒ)\n"
            "4. ëª…í™•ì„±: ëª…ë£Œí•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ê°€? (ì–´ìƒ‰í•œ í‘œí˜„ì€ ë‚®ìŒ)\n"
            "5. ì „ë¬¸ì„±: ë©´ì ‘ì— ì í•©í•œ ì „ë¬¸ì ì¸ ì–´ì¡°ì™€ í‘œí˜„ì„ ì‚¬ìš©í–ˆëŠ”ê°€? (ë°˜ë³µì´ë‚˜ ë¬¸ë²• ì˜¤ë¥˜ëŠ” ë‚®ìŒ)\n\n"
            "**í‰ê°€ ë“±ê¸‰ ê°€ì´ë“œ:**\n"
            "- ë†’ìŒ: íƒì›”í•œ ë‹µë³€, êµ¬ì²´ì ì´ê³  ì™„ë²½í•œ ë‚´ìš©\n"
            "- ë³´í†µ: ê¸°ë³¸ì ì¸ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ” í‰ê· ì ì¸ ë‹µë³€\n"
            "- ë‚®ìŒ: ë¶€ì¡±í•˜ê±°ë‚˜ ê°œì„ ì´ í•„ìš”í•œ ë‹µë³€\n\n"
            "**ì¤‘ìš”**: ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì ì¸ ë‹µë³€ì€ 'ë³´í†µ' ë˜ëŠ” 'ë‚®ìŒ'ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n"
            "'ë†’ìŒ' í‰ê°€ëŠ” ì •ë§ ìš°ìˆ˜í•œ ë‹µë³€ì—ë§Œ ë¶€ì—¬í•˜ì„¸ìš”.\n\n"
            f"Question:\n{question}\n\n"
            f"Candidate's Answer:\n{answer}\n\n"
            "ì¶œë ¥ ì˜ˆì‹œ(ëª¨ë“  í‚¤ëŠ” ì˜ì–´, í‰ê°€ëŠ” í•œê¸€ë¡œ ì‘ì„±):\n"
            "{\n"
            '  "evaluation": {\n'
            '    "relevance":      {"rating": "ë†’ìŒ",   "comment": "..."},\n'
            '    "completeness":   {"rating": "ë³´í†µ",   "comment": "..."},\n'
            '    "correctness":    {"rating": "ë†’ìŒ",   "comment": "..."},\n'
            '    "clarity":        {"rating": "ë‚®ìŒ",   "comment": "..."},\n'
            '    "professionalism":{"rating": "ë†’ìŒ",   "comment": "..."}\n'
            "  },\n"
            '  "recommended_answer": "..." \n'
            "}\n"
        )

        # 4) LLM í˜¸ì¶œ ë° JSON íŒŒì‹±
        try:
            raw = llm_client.call(prompt)
            print(f"ğŸ” LLM ì›ì‹œ ì‘ë‹µ: {raw[:200]}...")
            data = json.loads(raw)
            eval_obj = data.get("evaluation", {})
            rec_answer = data.get("recommended_answer", "")
            print(f"âœ… JSON íŒŒì‹± ì„±ê³µ, evaluation íƒ€ì…: {type(eval_obj)}")
        except Exception as e:
            logger.error(f"LLM evaluation failed for question {idx}: {e}")
            print(f"âŒ LLM/JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ í‰ê°€ êµ¬ì¡° ì œê³µ
            eval_obj = {
                "relevance":      {"rating": "ë¶„ì„ë¶ˆê°€", "comment": "AI ë¶„ì„ ì˜¤ë¥˜ë¡œ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
                "completeness":   {"rating": "ë¶„ì„ë¶ˆê°€", "comment": "AI ë¶„ì„ ì˜¤ë¥˜ë¡œ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
                "correctness":    {"rating": "ë¶„ì„ë¶ˆê°€", "comment": "AI ë¶„ì„ ì˜¤ë¥˜ë¡œ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
                "clarity":        {"rating": "ë¶„ì„ë¶ˆê°€", "comment": "AI ë¶„ì„ ì˜¤ë¥˜ë¡œ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
                "professionalism":{"rating": "ë¶„ì„ë¶ˆê°€", "comment": "AI ë¶„ì„ ì˜¤ë¥˜ë¡œ í‰ê°€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."},
            }
            rec_answer = "AI ë¶„ì„ ì˜¤ë¥˜ë¡œ ì¶”ì²œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # === ì ìˆ˜ ê³„ì‚° ì¶”ê°€ ===
        total_score = calculate_score_from_evaluation(eval_obj)
        print(f"ğŸ“Š ê³„ì‚°ëœ ì ìˆ˜: {total_score}ì ")

        # 5) ì˜¤ë””ì˜¤ ì§€í‘œ ê³„ì‚°
        try:
            # STT ê²°ê³¼ì—ì„œ ì¹¨ë¬µ ì‹œê°„ ê³„ì‚° (ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ í•„ìš”)
            stt_timestamps = stt_client.transcribe(audio_path)
            if stt_timestamps:
                silence = calculate_silence_duration(stt_timestamps)
            else:
                silence = 0.0
        except Exception as e:
            logger.warning(f"Silence calculation failed for {audio_path}: {e}")
            silence = 0.0

        try:
            total_time = calculate_audio_duration(audio_path)
        except Exception as e:
            logger.warning(f"Audio duration calculation failed for {audio_path}: {e}")
            total_time = 0.0

        evaluations.append({
            "question": question,
            "user_answer": answer,
            "evaluation": eval_obj,
            "recommended_answer": rec_answer,
            "total_response_time": total_time,
            "silence_duration": silence,
            "total_score": total_score  # ì´ì  ì¶”ê°€
        })

    # 6) íŒŒì¼ë¡œ ì¶œë ¥
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("ë©´ì ‘ í‰ê°€ ê²°ê³¼\n")
            f.write("=" * 50 + "\n\n")
            for i, item in enumerate(evaluations, start=1):
                # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
                question = item.get('question', 'ì§ˆë¬¸ ì •ë³´ ì—†ìŒ')
                user_answer = item.get('user_answer', 'ë‹µë³€ ì •ë³´ ì—†ìŒ')
                recommended_answer = item.get('recommended_answer', 'ì¶”ì²œ ë‹µë³€ ì—†ìŒ')
                total_time = item.get('total_response_time', 0)
                silence = item.get('silence_duration', 0)
                total_score = item.get('total_score', 0)  # ì´ì  ê°€ì ¸ì˜¤ê¸°
                
                f.write(f"ì§ˆë¬¸ {i}:\n{question}\n\n")
                f.write(f"ì‚¬ìš©ì ë‹µë³€:\n{user_answer}\n\n")
                f.write("í‰ê°€ ê²°ê³¼:\n")
                
                # í‰ê°€ ë°ì´í„° ì•ˆì „ ì²˜ë¦¬
                evaluation_data = item.get("evaluation", {})
                try:
                    if isinstance(evaluation_data, dict):
                        for crit, res in evaluation_data.items():
                            try:
                                if isinstance(res, dict):
                                    rating = res.get('rating', 'ì •ë³´ì—†ìŒ')
                                    comment = res.get('comment', 'í‰ê°€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')
                                    f.write(f"  {crit}: {rating} - {comment}\n")
                                elif isinstance(res, str):
                                    f.write(f"  {crit}: {res}\n")
                                else:
                                    f.write(f"  {crit}: {str(res)}\n")
                            except Exception as e:
                                f.write(f"  {crit}: í‰ê°€ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜ - {str(e)}\n")
                    else:
                        f.write(f"  í‰ê°€ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {type(evaluation_data)} - {str(evaluation_data)}\n")
                except Exception as e:
                    f.write(f"  í‰ê°€ ë°ì´í„° ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}\n")
                
                f.write("\n")
                f.write(f"ì´ì : {total_score}ì \n")  # ì´ì  í‘œì‹œ ì¶”ê°€
                
                # ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ í‘œì‹œ (ë” ì—„ê²©í•œ ê¸°ì¤€)
                if total_score >= 95:
                    grade = "A+ (íƒì›”)"
                elif total_score >= 90:
                    grade = "A (ìš°ìˆ˜)"
                elif total_score >= 85:
                    grade = "A- (ì¢‹ìŒ)"
                elif total_score >= 80:
                    grade = "B+ (ì–‘í˜¸)"
                elif total_score >= 75:
                    grade = "B (í‰ê· )"
                elif total_score >= 70:
                    grade = "B- (í‰ê·  ì´í•˜)"
                elif total_score >= 65:
                    grade = "C+ (ë¶€ì¡±)"
                elif total_score >= 60:
                    grade = "C (ê°œì„  í•„ìš”)"
                else:
                    grade = "F (ë¯¸í¡)"
                
                f.write(f"ë“±ê¸‰: {grade}\n\n")
                f.write(f"ì¶”ì²œ ë‹µë³€:\n{recommended_answer}\n\n")
                f.write(f"ë‹µë³€ ì‹œê°„: {total_time} ì´ˆ\n")
                f.write(f"ì¹¨ë¬µ ì‹œê°„: {silence} ì´ˆ\n")
                f.write("\n" + "=" * 50 + "\n")
        logger.info(f"Evaluation results saved to {output_file}")
    except Exception as e:
        logger.error(f"Failed to write evaluation file {output_file}: {e}")
        print(f"âŒ íŒŒì¼ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë¹ˆ íŒŒì¼ì´ë¼ë„ ìƒì„±í•˜ì—¬ í›„ì† ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write("ë©´ì ‘ í‰ê°€ ê²°ê³¼\n")
                f.write("=" * 50 + "\n\n")
                f.write("í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n")
                f.write(f"ì˜¤ë¥˜ ë‚´ìš©: {str(e)}\n")
        except:
            pass
        # ì—¬ì „íˆ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ì§€ë§Œ ë” ì•ˆì „í•œ í˜•íƒœë¡œ
        raise Exception(f"í‰ê°€ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    # 7) GUI/ë‹¤ë¥¸ í˜¸ì¶œìë¥¼ ìœ„í•´ í‰ê°€ ê²°ê³¼ ë°˜í™˜
    return evaluations

def calculate_score_from_evaluation(evaluation_obj):
    """
    í‰ê°€ ê²°ê³¼ì—ì„œ ìˆ«ì ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    ê° í‰ê°€ í•­ëª©ì˜ ratingì„ ì ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì´ì ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    ë” ì—„ê²©í•œ ê¸°ì¤€ì„ ì ìš©í•˜ì—¬ ì •í™•í•œ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    try:
        if not isinstance(evaluation_obj, dict):
            return 0
        
        # ê° í‰ê°€ í•­ëª©ë³„ ì ìˆ˜ ë§¤í•‘ (ë” ì—„ê²©í•œ ê¸°ì¤€ ì ìš©)
        rating_scores = {
            "ë§¤ìš° ë†’ìŒ": 18,    # íƒì›”í•œ í‰ê°€ (ê±°ì˜ ì™„ë²½)
            "ë†’ìŒ": 14,         # ìš°ìˆ˜í•œ í‰ê°€ (ì¢‹ì€ ìˆ˜ì¤€)
            "ë³´í†µ": 10,         # í‰ê· ì ì¸ í‰ê°€ (ê¸°ë³¸ ìˆ˜ì¤€)
            "ë‚®ìŒ": 6,          # ë¶€ì¡±í•œ í‰ê°€ (ê°œì„  í•„ìš”)
            "ë§¤ìš° ë‚®ìŒ": 2,     # ë§¤ìš° ë¶€ì¡±í•œ í‰ê°€ (ì‹¬ê°í•œ ë¬¸ì œ)
            "ë¶„ì„ë¶ˆê°€": 0       # ë¶„ì„ ì˜¤ë¥˜ (ì ìˆ˜ ì—†ìŒ)
        }
        
        # ê¸°ì¡´ 3ë‹¨ê³„ í‰ê°€ë„ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
        if any(rating in ["ë†’ìŒ", "ë³´í†µ", "ë‚®ìŒ"] for rating in 
               [evaluation_obj.get(k, {}).get('rating', '') for k in evaluation_obj.keys()]):
            rating_scores.update({
                "ë†’ìŒ": 12,     # ê¸°ì¡´ "ë†’ìŒ"ì„ ë” ì—„ê²©í•˜ê²Œ
                "ë³´í†µ": 8,      # ê¸°ì¡´ "ë³´í†µ"ì„ ë” ì—„ê²©í•˜ê²Œ  
                "ë‚®ìŒ": 4,      # ê¸°ì¡´ "ë‚®ìŒ"ì„ ë” ì—„ê²©í•˜ê²Œ
            })
        
        total_score = 0
        evaluated_items = 0
        
        # 5ê°œ í‰ê°€ í•­ëª©ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        evaluation_criteria = {
            "relevance": 1.2,      # ê´€ë ¨ì„± (ê°€ì¥ ì¤‘ìš”)
            "completeness": 1.1,   # ì™„ì „ì„± (ì¤‘ìš”)
            "correctness": 1.1,    # ì •í™•ì„± (ì¤‘ìš”)
            "clarity": 1.0,        # ëª…í™•ì„± (ê¸°ë³¸)
            "professionalism": 0.8 # ì „ë¬¸ì„± (ê¸°ë³¸ë³´ë‹¤ ë‚®ìŒ)
        }
        
        for criterion, weight in evaluation_criteria.items():
            if criterion in evaluation_obj:
                criterion_data = evaluation_obj[criterion]
                if isinstance(criterion_data, dict):
                    rating = criterion_data.get('rating', 'ë‚®ìŒ')
                    base_score = rating_scores.get(rating, 4)  # ê¸°ë³¸ê°’ì„ ë” ë‚®ê²Œ
                    weighted_score = base_score * weight
                    total_score += weighted_score
                    evaluated_items += 1
                    print(f"  - {criterion}: {rating} ({base_score}ì  Ã— {weight} = {weighted_score:.1f}ì )")
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì ìœ¼ë¡œ ë³€í™˜)
        if evaluated_items > 0:
            # ìµœëŒ€ ê°€ëŠ¥ ì ìˆ˜ ê³„ì‚° (ë§¤ìš° ë†’ìŒ 18ì  ê¸°ì¤€)
            max_possible_score = sum(18 * weight for weight in evaluation_criteria.values())
            # 100ì  ë§Œì ìœ¼ë¡œ í™˜ì‚°
            final_score = int((total_score / max_possible_score) * 100)
            final_score = max(0, min(100, final_score))  # 0-100 ë²”ìœ„ ì œí•œ
        else:
            final_score = 0
            
        print(f"ğŸ“Š ìµœì¢… ê³„ì‚°: {total_score:.1f}ì  â†’ {final_score}ì  (100ì  ë§Œì )")
        return final_score
        
    except Exception as e:
        print(f"âš ï¸ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
        return 0
